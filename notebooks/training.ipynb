{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Attend, Infer, Repeat (SQAIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "from os import path as osp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from sqair.experiment_tools import (load, init_checkpoint, parse_flags, get_session, print_flags,\n",
    "                                      print_num_params, print_variables_by_scope)\n",
    "from sqair import tf_flags as flags\n",
    "from sqair.eval_tools import bbox_colors, make_expr_logger, rect_stn\n",
    "from sqair.eval_tools import ProgressFig, make_logger\n",
    "from sqair.modules import SpatialTransformer as ST\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Flags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define flags\n",
    "\n",
    "flags.DEFINE_string('data_config', '../sqair/configs/seq_mnist_data.py', 'Path to a data config file.')\n",
    "flags.DEFINE_string('model_config', '../sqair/configs/mlp_mnist_model.py', 'Path to a model config file.')\n",
    "flags.DEFINE_string('results_dir', '../checkpoints', 'Top directory for all experimental results.')\n",
    "flags.DEFINE_string('run_name', 'test_run', 'Name of this job. Results will be stored in a corresponding folder.')\n",
    "\n",
    "flags.DEFINE_integer('batch_size', 32, '')\n",
    "\n",
    "flags.DEFINE_integer('log_itr', int(1e4), 'Number of iterations between storing tensorboard logs.')\n",
    "flags.DEFINE_integer('report_loss_every', int(1e3), 'Number of iterations between reporting minibatch loss - hearbeat.')\n",
    "flags.DEFINE_integer('save_itr', int(1e5), 'Number of iterations between snapshotting the model.')\n",
    "flags.DEFINE_integer('fig_itr', 10000, 'Number of iterations between creating results figures.')\n",
    "flags.DEFINE_integer('train_itr', int(2e6), 'Maximum number of training iterations.')\n",
    "flags.DEFINE_boolean('resume', False, 'Tries to resume the previous run if True.')\n",
    "flags.DEFINE_boolean('log_at_start', False, 'Evaluates the model between training commences if True.')\n",
    "flags.DEFINE_boolean('eval_on_train', True, 'Evaluates the model on the train set if True')\n",
    "\n",
    "flags.DEFINE_float('eval_size_fraction', 1., 'Fraction of the dataset to perform model evaluation on. Must be between'\n",
    "                                             '0. and 1.')\n",
    "\n",
    "flags.DEFINE_string('opt', 'rmsprop', 'Optimizer; choose from rmsprop, adam, sgd, momentum')\n",
    "flags.DEFINE_float('learning_rate', 1e-5, 'Initial values of the learning rate')\n",
    "flags.DEFINE_float('l2', 0.0, 'Weight for the l2 regularisation of parameters')\n",
    "flags.DEFINE_string('schedule', '4,6,10', 'Uses a learning rate schedule if True. Schedule = \\'4,6,10\\' '\n",
    "                                           'means that F.train_itr will be split in proportions 4/s, 6/s, 10/s,'\n",
    "                                           'where s = sum(schedule)')\n",
    "\n",
    "flags.DEFINE_boolean('test_run', False, 'Only a small run if True')\n",
    "flags.DEFINE_string('gpu', '0', 'Id of the gpu to use for this job.')\n",
    "flags.DEFINE_boolean('debug', False, 'Adds a lot of tensorboard summaries if True.')\n",
    "\n",
    "F = flags.FLAGS\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = F.gpu\n",
    "\n",
    "# Parse flags\n",
    "parse_flags()\n",
    "F = flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = osp.join(F.results_dir, F.run_name)\n",
    "logdir, flags, resume_checkpoint = init_checkpoint(logdir, F.data_config, F.model_config, F.resume)\n",
    "checkpoint_name = osp.join(logdir, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph and Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "data_dict = load(F.data_config, F.batch_size)\n",
    "\n",
    "# mean img\n",
    "imgs = data_dict.train_data.imgs\n",
    "mean_img = imgs.mean(tuple(range(len(imgs.shape) - 2)))\n",
    "assert len(mean_img.shape) == 2\n",
    "\n",
    "try:\n",
    "    coords = data_dict.train_coord\n",
    "except AttributeError:\n",
    "    coords = None\n",
    "\n",
    "model = load(F.model_config, img=data_dict.train_img, coords=coords, num=data_dict.train_num, mean_img=mean_img, debug=F.debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "lr = F.learning_rate\n",
    "if F.schedule:\n",
    "    schedule = [float(f) for f in F.schedule.split(',')]\n",
    "    schedule = np.cumsum(schedule)\n",
    "    schedule = schedule * F.train_itr / schedule[-1]\n",
    "    schedule = list(np.round(schedule).astype(np.int32))\n",
    "    lrs = list(lr * (1./3) ** np.arange(len(schedule)))\n",
    "    print lrs, schedule\n",
    "    lr = tf.train.piecewise_constant(global_step, schedule[:-1], lrs)\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "\n",
    "opt = F.opt.lower()\n",
    "if opt == 'rmsprop':\n",
    "    opt = tf.train.RMSPropOptimizer(lr, momentum=.9)\n",
    "elif opt == 'adam':\n",
    "    opt = tf.train.AdamOptimizer(lr)\n",
    "elif opt == 'sgd':\n",
    "    opt = tf.train.GradientDescentOptimizer(lr)\n",
    "elif opt == 'momentum':\n",
    "    opt = tf.train.MomentumOptimizer(lr, momentum=.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, gvs = model.make_target(opt, n_train_itr=F.train_itr, l2_reg=F.l2)\n",
    "tf.summary.scalar('target', target)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_step = opt.apply_gradients(gvs, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = get_session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore session if Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.restore_from_vae(sess)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=10000)\n",
    "if resume_checkpoint is not None:\n",
    "    print \"Restoring checkpoint from '{}'\".format(resume_checkpoint)\n",
    "    saver.restore(sess, resume_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "all_summaries = tf.summary.merge_all()\n",
    "\n",
    "# Setup hearbeat reports\n",
    "report = [target, model.normalised_elbo_iwae, model.num_steps, model.num_step_accuracy, tf.shape(data_dict.train_img)[0]]\n",
    "names = 'target iwae num_steps num_step_acc seq_len'.split()\n",
    "report = {k: v for k, v in zip(names, report)}\n",
    "maybe_report = 'num_disc_steps num_prop_steps'.split()\n",
    "\n",
    "for k in maybe_report:\n",
    "    try:\n",
    "        report[k] = getattr(model, k)\n",
    "    except AttributeError:\n",
    "        print 'Skipping report: \"{}\"'.format(k)\n",
    "\n",
    "ax = data_dict['axes']['imgs']\n",
    "factor = F.eval_size_fraction\n",
    "train_batches, valid_batches = [int(data_dict[k]['imgs'].shape[ax] * factor / F.batch_size) for k in ('train_data', 'valid_data')]\n",
    "\n",
    "log = make_logger(model, sess, summary_writer, data_dict.train_tensors,\n",
    "                  train_batches, data_dict.valid_tensors, valid_batches, F.eval_on_train)\n",
    "\n",
    "try:\n",
    "    progress_fig = ProgressFig(model, sess, logdir, seq_n_samples=16)\n",
    "    img_summaries = None\n",
    "except:\n",
    "    progress_fig = None\n",
    "    img_summaries = model.img_summaries()\n",
    "\n",
    "\n",
    "def try_plot(itr):\n",
    "    if progress_fig is not None:\n",
    "        progress_fig.plot_all(itr)\n",
    "    else:\n",
    "        summaries = sess.run(img_summaries)\n",
    "        summary_writer.add_summary(summaries, train_itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_itr = sess.run(global_step)\n",
    "print 'Starting training at iter = {}'.format(train_itr)\n",
    "\n",
    "if F.log_at_start or train_itr == 0:\n",
    "    log(train_itr)\n",
    "    try_plot(train_itr)\n",
    "\n",
    "# Train!\n",
    "while train_itr < F.train_itr:\n",
    "    l, train_itr, _ = sess.run([report, global_step, train_step])\n",
    "\n",
    "    if train_itr % F.report_loss_every == 0:\n",
    "        print '{}: {}'.format(train_itr, str(l)[1:-1].replace('\\'=', ''))\n",
    "        summaries = sess.run(all_summaries)\n",
    "        summary_writer.add_summary(summaries, train_itr)\n",
    "\n",
    "    if train_itr % F.log_itr == 0:\n",
    "        log(train_itr)\n",
    "\n",
    "    if train_itr % F.save_itr == 0:\n",
    "        saver.save(sess, checkpoint_name, global_step=train_itr)\n",
    "\n",
    "    if train_itr % F.fig_itr == 0:\n",
    "        try_plot(train_itr)\n",
    "\n",
    "saver.save(sess, checkpoint_name, global_step=train_itr)\n",
    "try_plot(train_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
